{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import json\n",
    "from community import community_louvain\n",
    "from infomap import Infomap\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticNetworkAnalysis:\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "        self.corpus, self.tokens = self.prepare_corpus()\n",
    "        self.vectorizer, self.tfidf_matrix = self.calculate_tfidf(min_df=0)\n",
    "        self.vocab = self.vectorizer.get_feature_names_out()\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def prepare_corpus(self):\n",
    "        corpus = []\n",
    "        tokens = []\n",
    "        for post in tqdm(self.data, desc=\"Preparing corpus\"):\n",
    "            corpus.append(' '.join(post['title_tokens']))\n",
    "            tokens.append(post['title_tokens'])\n",
    "            for comment in post['comments']:\n",
    "                corpus.append(' '.join(comment['body_tokens']))\n",
    "                tokens.append(comment['body_tokens'])\n",
    "        return corpus, tokens\n",
    "\n",
    "    def calculate_tfidf(self, min_df=10):\n",
    "        vectorizer = TfidfVectorizer(min_df=min_df)\n",
    "        tfidf_matrix = vectorizer.fit_transform(tqdm(self.corpus, desc=\"Calculating TF-IDF\"))\n",
    "        return vectorizer, tfidf_matrix\n",
    "\n",
    "    def co_occurrence_matrix_with_tfidf(self, window_size=5):\n",
    "        vocab_size = len(self.vocab)\n",
    "        matrix = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "        word_to_index = {word: i for i, word in enumerate(self.vocab)}\n",
    "\n",
    "        for token_list, row_tfidf in tqdm(zip(self.tokens, self.tfidf_matrix), total=len(self.tokens), desc=\"Computing co-occurrence matrix\"):\n",
    "            for i in range(len(token_list) - window_size + 1):\n",
    "                pairs = itertools.combinations(token_list[i:i + window_size], 2)\n",
    "                for u, v in pairs:\n",
    "                    if u in word_to_index and v in word_to_index:\n",
    "                        index_u = word_to_index[u]\n",
    "                        index_v = word_to_index[v]\n",
    "                        tfidf_u = row_tfidf[0, index_u]\n",
    "                        tfidf_v = row_tfidf[0, index_v]\n",
    "                        matrix[index_u][index_v] += (tfidf_u * tfidf_v)\n",
    "                        matrix[index_v][index_u] += (tfidf_u * tfidf_v)\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    def create_semantic_network(self, co_occurrence_matrix, min_weight=6):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        for i, word1 in tqdm(enumerate(self.vocab), desc=\"Creating semantic network\", total=len(self.vocab)):\n",
    "            for j, word2 in enumerate(self.vocab):\n",
    "                if i != j and co_occurrence_matrix[i][j] >= min_weight:\n",
    "                    G.add_edge(word1, word2, weight=co_occurrence_matrix[i][j])\n",
    "\n",
    "        return G\n",
    "\n",
    "    def normalize_edge_weights(self, G):\n",
    "        max_edge_weight = max([d['weight'] for u, v, d in G.edges(data=True)])\n",
    "        for u, v, d in tqdm(G.edges(data=True), desc=\"Normalizing edge weights\"):\n",
    "            d['normalized_weight'] = d['weight'] / max_edge_weight\n",
    "\n",
    "    def export_to_gephi(self, G, filename):\n",
    "        nx.write_gexf(G, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \\'__main__\\':\\n    analysis = SemanticNetworkAnalysis(\\'serialkillers_cleaned.json\\')\\n    co_occurrence_with_tfidf = analysis.co_occurrence_matrix_with_tfidf(window_size=6)\\n    semantic_network = analysis.create_semantic_network(co_occurrence_with_tfidf, min_weight=8)\\n    analysis.normalize_edge_weights(semantic_network)\\n    analysis.export_to_gephi(semantic_network, \"serialkillers_tfidf.gexf\")'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''if __name__ == '__main__':\n",
    "    analysis = SemanticNetworkAnalysis('serialkillers_cleaned.json')\n",
    "    co_occurrence_with_tfidf = analysis.co_occurrence_matrix_with_tfidf(window_size=6)\n",
    "    semantic_network = analysis.create_semantic_network(co_occurrence_with_tfidf, min_weight=8)\n",
    "    analysis.normalize_edge_weights(semantic_network)\n",
    "    analysis.export_to_gephi(semantic_network, \"serialkillers_tfidf.gexf\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \\'__main__\\':\\n    analysis = SemanticNetworkAnalysis(\\'serialkillers_cleaned.json\\')\\n    co_occurrence_with_tfidf = analysis.co_occurrence_matrix_with_tfidf(window_size=8)\\n    semantic_network = analysis.create_semantic_network(co_occurrence_with_tfidf, min_weight=4)\\n    analysis.normalize_edge_weights(semantic_network)\\n    analysis.export_to_gephi(semantic_network, \"serialkillers_tfidf.gexf\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''if __name__ == '__main__':\n",
    "    analysis = SemanticNetworkAnalysis('serialkillers_cleaned.json')\n",
    "    co_occurrence_with_tfidf = analysis.co_occurrence_matrix_with_tfidf(window_size=8)\n",
    "    semantic_network = analysis.create_semantic_network(co_occurrence_with_tfidf, min_weight=4)\n",
    "    analysis.normalize_edge_weights(semantic_network)\n",
    "    analysis.export_to_gephi(semantic_network, \"serialkillers_tfidf.gexf\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing corpus: 100%|██████████| 965/965 [00:00<00:00, 20899.94it/s]\n",
      "Calculating TF-IDF: 100%|██████████| 79102/79102 [00:00<00:00, 135650.74it/s]\n",
      "Computing co-occurrence matrix: 100%|██████████| 79102/79102 [02:37<00:00, 502.46it/s]\n",
      "Creating semantic network: 100%|██████████| 44527/44527 [29:09<00:00, 25.46it/s]\n",
      "Normalizing edge weights: 100%|██████████| 2967/2967 [00:00<00:00, 2202566.37it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    analysis = SemanticNetworkAnalysis('unresolved_cleaned.json')\n",
    "    co_occurrence_with_tfidf = analysis.co_occurrence_matrix_with_tfidf(window_size=4)\n",
    "    semantic_network = analysis.create_semantic_network(co_occurrence_with_tfidf, min_weight=4)\n",
    "    analysis.normalize_edge_weights(semantic_network)\n",
    "    analysis.export_to_gephi(semantic_network, \"unresolved_tfidf2.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
